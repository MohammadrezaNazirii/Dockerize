services:
  llm-inference-service:
    image: solidmann/llm-inference-image:latest  # Use your own image (that already pushed to docker hub)
    container_name: llm-inference-container
    ports:
      - "8080:5000" # Host port 8080 maps to container port 5000
    environment:
      - METRICS_LOG_FILE=/app/inside_compose_inference_metrics.csv
      # Set the `METRICS_LOG_FILE` value to be `inside_compose_inference_metrics.csv`
    volumes:
      - ./compose_inference_metrics.csv:/app/inside_compose_inference_metrics.csv 
      # Mount volume to get output in you host machine in a file named `compose_inference_metrics.csv`
